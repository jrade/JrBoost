simple buffer management API with capacity() and clear() functions
	look more into memory issues (see Higgs Notes)

TreeTrainerImpl:

	parallelize updateSampleStatus   [faster update sample status]
		separate updateSums_() function, not parallelized
		at level 0:
			initRoot_()
			initSampleStatus_()
			initSumsLevel0_()
		at other levels:
			finalizeNodeTrainers_()
			updateSampleStatus()
			initSums_()
		maybe name everything at level 0 init and at other levels update
			updateSaveMemory

	outdata and weights interleaved in a single array   [faster update splits]  
		Array2f and Array2Xf types
		each entry could be { w, w * y }
		this array could be created in BoostTrainer inside the loop that calculates w and y
		use row major eigen array with fixed second dimension, or just a vector of pairs?
		after this, test logit boost again

	make sure right child always = left child + 1?    [faster tree predict]
		will let us optimize predict code, update status code
		requires breadth first storage always
			better use a breadth first file format for tree classifiers
		run some preliminary tests before implementing the whole thing

better stratification when training multigroup classifiers

write some simple documentation of the different boost methods
	then test logit boost with   f' / (f'' + delta * f' * f')?

are the submatrix functions needed at all?
	or are they a useless optimization based on faulty profiling data?
		profile again with TCGA data

------------------------------------------------------------------------------------------------------------------------

major tasks:
	release 0.1 (reset file format version?)
	improved Higgs example (see "Higgs Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	BART
	improved Otto example (see "Otto Notes.txt")
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	histogram-based tree builder (would speed up Otto a lot, Higgs also)
	multinomial predictors
	documentation of regularized logit boost method

features to add:
	simpler high level API that does cross-validation and applies preprocessing
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	set seed to make the code deterministic (how to handle dynamic scheduling? doable, but a bit tricky)
	U-test (with correct handling of ties)
	multiple inData and multiple variable counts (to handle feature engineering)

optimizations (accuracy):
	test the new selectVariablesByLevel option
	boosted forests
		maybe train with forestSize = 1, then build with a larger forestSize
	boost stop criteria
	pruning
	multi-label stratification when generating folds and selecting active samples
	other ways of regularizing logit boost?
	
optimizations (speed):
	improved profiling log
		group into unthreaded, partially threaded, fully threaded and thread synch
	t-test and F-test:
		divide the variables into large blocks, one for each thread
		then divide the variables into subblocks of 4 each (__m128 that will be cast to __m256d)
	improved cost function when ordering options objects in parallel train
	parallelize predict function?? (omp parallellize ensemble and boost predictors, but only in nonparallel regions)
	make TreeTrainerImpl::updateSampleStatus_() code branchfree by introducing dummy nodes that contain the unused samples
		maybe maintain sorted lists of active samples to speed this up further
	look into fast/precise sums; does it make any difference for speed and accuracy?
	PGO
	Test with other compilers:
		can they autovectorize fast exp (then the manually vectorized code is not needed)
		can they handle 128 threads?
		does the Intel compiler produce code that runs fast on an AMD processor

minor things:
	replace size_t by uint64_t throghout the code (32-bit build not supported, nor 128-bit)
	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
	is there any need for the fudge term in the t- and F-test? (How large should the fudge term be?)
	save and load:
		check that dataset does not have more than 2G variables
		get rid of uint32 nonsense?
	compare performance (speed and accuracy) with xgboost
	clang code formatting
	how fast can a column major implementation of the t-statistic be?
	look more into Eigen arrays and std::data
	seems pybind11 does not support arguments of types such as optional<reference_wrapper<vector<int>>>
		that would be useful for passing sample lists to t-test and F-test
	runtime checks for AVX2 and AVX512
	Eigen:
		why does std::data() not work here? (see Select.cpp)
			const float* pInDataRowI = std::data(inData.row(inI));
			float* pOutDataRowI = &outData(outI, 0);

