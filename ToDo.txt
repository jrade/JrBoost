prioritized:

	refactor 
		TreeTrainerBase -> TreeTrainer, TreeTrainer->TreeTrainerImpl
			TreeTrainer::createInstance
			get rid of ForestTrainer
			add a TreeTrainerBuffers struct
			update UML diagrams

	profile:
		revive and use the Leukemia train and eval script
		file size with and w/o gain again
			resurrect the Leukemia train script and train with that
		packed data
			why does inner thread synch take so much time with packed data?
				do some more detailed profiling?
				what happens if there are no inner threads?
					push things to max with 128 threads, no inner threads on fast PC
			test with Higgs cv on Laptop
			if we keep it, don't pass around outData and weights
		do we need the drop py::arg().noconvert() in t-test and F-test python wrappers?

	review new stratification code
		test with leukemia

	remove support for old file format versions
		throw exception if they are detected
			but first decide how to handle gain

	release 0.1

	do everything under minor things

..............................................................

major tasks:
	improved Higgs example (see "Higgs Notes.txt")
	simpler high level API that does cross-validation and applies preprocessing
	documentation of the different boost methods, incuding regularized logit boost
	improved Otto example (see "Otto Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	histogram-based tree builder (would speed up Otto a lot, maybe Higgs too)
	BART
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	multinomial predictors

features to add:
	U-test (with correct handling of ties)
	multiple indata and multiple variable counts (to handle feature engineering)
		applications: PCA, separate counts for up- and down-regulated variables, chromosomes, NCC
	new way of calculating variable importance weights: when predicting (instead of when training)

optimizations (accuracy):
	iterate from both ends and meet in the middle when finding best split
	better parallelTrainAndEval for non-additive loss function (such as AUC)
	boost stop criteria
	pruning
	other ways of regularizing logit boost (e.g. f' / (f'' + delta * f' * f') which is scale-invariant)
	
optimizations (speed):
	parallelize:
		ForestTrainer::train
		TreeTrainer::updateSampleStatus_
			easier if we do not use exact sums; can we do that?
		predict functions
	make branch free:
		TreeTrainer::updateSampleStatus_:
			introducing dummy nodes that contain the unused samples
			maybe maintain sorted lists of active samples to speed this up further
	t-test and F-test: are they SIMD optimized? do we load eacg cache line twice for large data sets?
	look into fast/precise sums; does it make any difference for speed and accuracy?
	have boost trainer produce packed wy data and feed it to tree trainer (some SIMD swizzle magic needed?)
	PGO

later:
	Clang format: test BreakBeforeClosingParen when it is released

minor things:
	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
	pybind11:
		seems arguments of types such as optional<reference_wrapper<vector<int>>> are not supported
			that would be useful for passing sample lists to t-test and F-test
		not supported:
			py::class_<Predictor, shared_ptr<const Predictor>>{ mod, "Predictor" }
			py::class_<const Predictor, shared_ptr<const Predictor>>{ mod, "Predictor" }
			also, unique_ptr to const T
	Eigen:
		why cannot we use (a < inf).all() to detect NaN values in Eigen arrays?
		why does std::data() not work here? (see Select.cpp)
			const float* pInDataRowI = std::data(inData.row(inI));
			float* pOutDataRowI = &outData(outI, 0);
	compare performance (speed and accuracy) with xgboost
		construct Python classes with same interface as jrboost python classes

