save my handwritten notes about regularized logit boost

https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

save and load classifier
	binary format
		byte 1 classifier type
		byte 2 version mumber of the format

get rid of vector class library
	too complex and may be bad with a better compiler
	select one implementation each of exact and fast implementations

test accuracy of fast math
	1. improved fast math with smarter (x + 1) ^ (gamma-2)
	2. update the Gist with fastLog and fastPow
	3. plot min and max rel error of fastPow as a function of gamma
		let x range over 2 ** -n to 2 **n; seems the exact form of curve depends a little on on
		send a mail to Nic with the results

try clang/llvm compiler plugin

get Higgs, Otto and Iris to run again

write some notes about regularized logit boost
	just some brief comments in the code?

need to move on with
	trees
	histogram based stumps (would speed up Otto a lot)
	gradient boost
	BART

========================================================================
	
General:
	optimizations:
		optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
		profile Higgs and otto with float versus double outData and weights - decide which to use

	test:
		test util.stratifiedRandomSplit
		write some tests for the loss functions
			feed random data into them and check that lor version on lor data and p version and p data give same thing
			check limit behaviour of regularized log loss as gamma -> 0 or 1

Higgs:
	run with LOR and median
	plot the result
	draw a line in the plot that marks the estimated cutoff
	should we use a 0 cutoff?
	implement the AMS loss function
	run with CV - ignore the test datasets

Higgs:
	AUC as optimization target
		better cv + AUC implementation?
	percentage threshold

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc


	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

better understanding of boosting:
	log high sample weight ratio
	log each sample after each boost iteration in a big spread sheet
	how does boosted trees behave if we only use trivial predictors?

-----------------------------------------------------------------------

trees
	allocate nodes with memory pool
	also pruning

histogram based stumps (would speed up Otto a lot)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

test PGO

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

Kaggle Higgs dataset:
	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

BART??

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)
	multi-group
	validation of indata to predict?

======================================================================================

PyBind11 issues:
	how translate custom C++ exceptions to standard Python exceptions
	no way of specifying noconvert for property setters
	how to set function attributes
	
Gerstmann issues:
	seed function would be useful
	free functions (operator== and operator!=) should be inline
		or linker will complain if you include the header in multiple translation units
	should be standard compliant

Eigen issues:
	can not reset Ref objects
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...

	Eigen::Ref<Eigen::ArrayXf> u;
	auto p1 = u.begin();
	auto p2 = begin(u);
	auto p3 = u.cbegin();
	auto p4 = cbegin(u);
	float* p5 = &u(0);

	Eigen::Ref<Eigen::ArrayXf> u;
	auto q1 = u.cbegin();
	auto q2 = cbegin(u);
	const float* q3 = &u(0);

	also reverse iterators

NumPy issues:
	inData[trainSamples, :] does not preserve Fortran (column major) order
		how to get fortran ordered slices of fortran ordered arrays?

Pandas issues:
	if you set dtype when calling read_csv(), this is applied to the index column as well 

------------------------------------------------------------------------

take a look at std::span, std::view, std::mdview, ranges etc in C++20

======================================================================================

Notes:

/O2 =  /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")


