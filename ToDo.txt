improve interface:
	C++ code should use Options objects
	Python code should use dict objects
	glue layer takes
		map<string, py::object>
		vector<map<string, py::object>
	and converts map<string, py::object> -> BoostOptions

cleanup of minimize functions
	turn then into completely generic minimization functions
	
documentation of regularized logit boost

new GitHub login method
	https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

get Higgs, Otto and Iris to run again

Eigen:
	read Eigen 3.4 docs
	report that
		ArrayXf a = {0, 4, 5};
	compiles
	pass Ref<const Array> by const ref?

========================================================================

need to move on with
	trees
	histogram based stumps (would speed up Otto a lot)
	gradient boost
	BART

========================================================================
	
General:
	optimizations:
		optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
		profile Higgs and otto with float versus double outData and weights - decide which to use

Higgs:
	run with LOR and median
	plot the result
	draw a line in the plot that marks the estimated cutoff
	should we use a 0 cutoff?
	implement the AMS loss function
	run with CV - ignore the test datasets
	AUC as optimization target
		better cv + AUC implementation?
	percentage threshold

	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc

	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

alternative implementation of sortedUsedSamples
	select fastest implementation dynamically depending on sample count etc.
	maybe do this as a separate stump trainer class

U-test (with correct handling of ties)

trees
	allocate nodes with memory pool (use std::pmr::monotonic_buffer_resource)
	also pruning

histogram based stumps (would speed up Otto a lot)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

multiple inData and multiple variableCount (to handle feature engineering etc.)

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)

	test accuracy of fast math
		1. improved fast math with smarter (x + 1) ^ (gamma-2)
		2. update the Gist with fastLog and fastPow
		3. plot min and max rel error of fastPow as a function of gamma
			let x range over 2 ** -n to 2 **n; seems the exact form of curve depends a little on on
			send a mail to Nic with the results

======================================================================================
	
Eigen issues:
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

======================================================================================

Notes:

/O2 =  /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")


