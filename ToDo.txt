TreeTrainerImpl:

	tlOrderedSampleIndex_ -> orderedSamplesByNode

	do a careful comparison of the current and the discarded (flexible threading, other branch in repo) code
		port everything that looks useful
		check how the main tree builder loop is structured in the discarded code

	update sumW and sumWY in updateSampleSttaus()
		write sampleCount, sumW and sumWY directly in nodes
		need separate update of root
		at the same time, look into updates of tlSampleCountByParentNode_ and tlSampleCountByChildNode_
			replace them by sampleCount members in the Node objects?

	profile the two implementations
		compare the detaled profile logs
		is the alt implementation faster?
		put together suitable benchmarks
			Leukemia build-and-validate ?
			higgs.py and higgs_quick.py ?
			TCGA500 validate (only on desktop) ?
			store in a file called Benchmark.txt ?

	small things:
		sneaky optimization of updateOrderedSamples using (s[i] & 1) ? ... : ...
		use std::span to simplify code?

	notes:
		the alt implementation can not be used with variables per level
		the alt implementation should not be used with depth 1 (would waste a lot of memory)


Higgs example:

	feature engineerings
		read paper appendix A, then appendix B
		look at winning (and other) submissions

	target loss functions (for fitting hyper parameters):
		AUC:      3.7372 -> 3.7403
		AMS:      3.7340 -> 3.7342
		ll(1e-3): 3.7156 -> 3.7073
	
	more robust AMS cutoff fitting (testing every possible cutoff may lead to overfitting, apply some smoothing?)
	also: test simple cutoff again
	population optimization parameters (generator approach)
	repetition count

	minGain feature
	forward and backward pruning
	drop useless variables?

	cross-validation for fitting hyper parameters and AMS cutoff
		only use the data files train.csv and test.csv from Kaggle competition
		trainer that works with sample subset

	look at final version of my Kaggle code
	look at winning submissions, paper, code at https://github.com/melisgl/higgsml
	look at other publicly available submissions
		
	do some profiling
		how does it scale with number of cores (using the full dataset)
		what takes so much time at the end?

	compare with the xgbost higgs example:
		speed, accuracy?

	how does higgs.py find jrboost when importing it? (this ís a bit of a mystery)

------------------------------------------------------------------------------------------------------------------------

major tasks:
	improved Otto example
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	BART
	other parallelization strategy
		the current version is threadsafe but non threaded
		first put together a simple non-threadsafe version
		then from there a threaded version
		finally a nested version
			https://software.intel.com/content/www/us/en/develop/videos/using-nested-parallelism-in-openmp.html
	histogram-based tree builder (would speed up Otto a lot)
	multinomial predictors

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	set seed (to make the training process deterministic)
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	used variable subset by level option (may be expensive)
	multiple inData and multiple variable counts (to handle feature engineering etc.)
	U-test (with correct handling of ties)

minor things:
	predict overload that takes a one-dim array and returns a double (use in QD Demo)
	is there any need for the fudge term in the t- and F-test? 
		How large should the fudge term be?
	documentation of regularized logit boost
	review handling of rounding errors towards end when splitting nodes (look in old code)
	clang code formatting
	what is the correct way of adding terms when calculating variable weights?
	have the predictor class constructors take float instead of double as appropriate
	test pragma omp threadlocal

miscellaneous:
	test pruning again (also with abs limit)
	trick to reduce rounding off errors towards the end
		calculate right sums as total - left instead of doing it incrementally
		then we should have right right sum == 0.0 at the end (assert this?)
	are static thread_local buffers worth the trouble? some of them? profile
	if we allowed depth = 0 (and why wouldn't we?), then the root would not get properly initialized
	let Split class find the splits??
		can also update the tree - see the old TreeTrainerAlt / NodeTrainer code
	compare performace (speed and accuracy) with xgboost

optimizations:
	test PGO again
	test narrower type for strata
	float or double for outdata and weights?
	optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
	alternative implementation of sortedUsedSamples
		select fastest implementation dynamically depending on sample count etc.
		maybe do this as a separate stump trainer class

------------------------------------------------------------------------------------------------------------------------

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?
	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc
	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

review use of integer and floating point data types
	use int64_t as default integer type?
		bow we got sszize_t and ssize()

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

Eigen issues:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles but gives unexpected result
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

------------------------------------------------------------------------------------------------------------------------

Notes:

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
