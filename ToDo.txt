
add fast track to alpha boost

send a mail to Nic Schraudolph and ask about the fastPow

need underflow checks?
	do a bit of logging
		log every record, including iteration count
	
SIMD implementation of fastExp etc?
	read https://info.ornl.gov/sites/publications/files/Pub69214.pdf

-----------------------------------------------------------------------------

reenable checks of outData and weights in StumpTrainerImpl??
reenable sumW = 0 warning?

alpha and logit boost
	double check the calculations behind alpha boost again
		w* = w * phi'',  y* = -phi' / phi''
	write some notes about alpha boost
	get logit boost to work as well

General:
	restore sumW = 0 warning in StumpTrainerImpl
	
	optimizations:
		boosting code is slow for alpha and logit boost:
			rewrite uisng simd (OpenMP or intrinsics)?
			does schraudolph hurt accuracy?
			https://github.com/jhjourdan/SIMD-math-prims
			http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html

		optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
		profile Higgs and otto with float versus double outData and weights - decide which to use
		test with Clang?
			https://docs.microsoft.com/en-us/cpp/build/clang-support-msbuild?view=msvc-160

	test:
		test util.stratifiedRandomSplit
		write some tests for the loss functions
			feed random data into them and check that lor version on lor data and p version and p data give same thing
			check limit behaviour of alpa loss as alpha -> 0 or 1

Higgs:
	run with LOR and median
	plot the result
	draw a line in the plot that marks the estimated cutoff
	should we use a 0 cutoff?
	implement the AMS loss function
	run with CV - ignore the test datasets

Higgs:
	AUC as optimization target
		better cv + AUC implementation?
	percentage threshold

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test alpha loss with some alpha around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc


	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

==================================================================

main                      1404   1.2%
  t-rank                  4436   3.9%    4.0
  train boost            13200  11.7%   15.8
  train stumps
    used samples          6897   6.1%    8.3
    used variables        3613   3.2%   10.0
    sums                   498   0.4%    4.1
    sorted used s.       56794  50.1%    2.5
    best split           23609  20.8%    7.3
  predict                 1245   1.1%    6.0
  OMP barrier              250   0.2%
  dyn. memory             1358   1.2%  192.5

zero calibration: 65.2
profiling overhead: 24.0%
slow branch: 2.5%

0:01:10

==================================================================

using size_t and double everywhere
	except:
		arrays of sampleIndices use smallest type
		inData: float

	profile the first decision again with a larger dataset (Higgs for instance)

=====================================================================

better understanding of boosting:
	log high sample weight ratio
	log each sample after each boost iteration in a big spread sheet
	how does boosted trees behave if we only use trivial predictors?

-----------------------------------------------------------------------

python package organization

trees
	allocate nodes with memory pool
	also pruning

histogram based stumps (would speed up Otto a lot)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

test PGO

extended algorithms:
	check if the alpha boost idea works
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

Kaggle Higgs dataset:
	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

BART??

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)
	multi-group
	validation of indata to predict?

----------------------------------------------------------------------

PyBind11 issues:
	how translate custom C++ exceptions to standard Python exceptions
	no way of specifying noconvert for property setters
	how to set function attributes
	
Gerstmann issues:
	seed function would be useful
	free functions (operator== and operator!=) should be inline
		or linker will complain if you include the header in multiple translation units
	should be standard compliant

Eigen issues:
	can not reset Ref objects
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...

	Eigen::Ref<Eigen::ArrayXf> u;
	auto p1 = u.begin();
	auto p2 = begin(u);
	auto p3 = u.cbegin();
	auto p4 = cbegin(u);
	float* p5 = &u(0);

	Eigen::Ref<Eigen::ArrayXf> u;
	auto q1 = u.cbegin();
	auto q2 = cbegin(u);
	const float* q3 = &u(0);

	also reverse iterators

NumPy issues:
	inData[trainSamples, :] does not preserve Fortran (column major) order
		how to get fortran ordered slices of fortran ordered arrays?

Pandas issues:
	if you set dtype when calling read_csv(), this is applied to the index column as well 

------------------------------------------------------------------------

take a look at std::span, std::view, std::mdview, ranges etc in C++20

======================================================================================

Notes:

/O2 =  /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")


