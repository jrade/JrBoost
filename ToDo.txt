Play around with alpha-boost

implement logit boost
	is it stable or do we need to handle overflow
	is alpha boost stabel for very small alpha?
	log Fy

write some notes about alpha boost

need to move on with
	trees
	histogram based stumps (would speed up Otto a lot)
	gradient boost
	BART

.............................................................................................

speed optimization:
	test the accuracy of:
		exact math
		fast math
		improved fast math with smarter (x + 1) ^ (alpha-2)

	if fast math is not good enough:
		there are alternatives to the Schraudolph fast exp:
			https://github.com/jhjourdan/SIMD-math-prims
		maybe easier to write code does not mess around with lots of casts
		also:
			http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html
			http://www.spfrnd.de/posts/2018-03-10-fast-exponential.html
		then there is a fast(?) exp in Agners library

	get rather poor performance from VCL
		test libsimdpp instead?
			https://p12tic.github.io/libsimdpp/v2.1/libsimdpp/w/index.html
		use SIMD intrinsics
		compile with Clang/GCC?

	contact Agner Fog:
		report typo, manual p. 34 top row 
		a block size macro would be useful when testing alignment
		mention fast exp

		1. test with clang instead - seems it might do a better job of vectorizing?
				https://docs.microsoft.com/en-us/cpp/build/clang-support-msbuild?view=msvc-160

		2. update the Gist with fastLog and fastPow
	
		3. plot min and max rel error of fastPow as a function of alpha
			let x range over 2 ** -n to 2 **n; seems yhe exact form of curve depends a little on on
			send a nail to Nic with the results

========================================================================
	
General:
	optimizations:
		optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
		profile Higgs and otto with float versus double outData and weights - decide which to use

	test:
		test util.stratifiedRandomSplit
		write some tests for the loss functions
			feed random data into them and check that lor version on lor data and p version and p data give same thing
			check limit behaviour of alpa loss as alpha -> 0 or 1

Higgs:
	run with LOR and median
	plot the result
	draw a line in the plot that marks the estimated cutoff
	should we use a 0 cutoff?
	implement the AMS loss function
	run with CV - ignore the test datasets

Higgs:
	AUC as optimization target
		better cv + AUC implementation?
	percentage threshold

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test alpha loss with some alpha around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc


	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

better understanding of boosting:
	log high sample weight ratio
	log each sample after each boost iteration in a big spread sheet
	how does boosted trees behave if we only use trivial predictors?

-----------------------------------------------------------------------

python package organization

trees
	allocate nodes with memory pool
	also pruning

histogram based stumps (would speed up Otto a lot)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

test PGO

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

Kaggle Higgs dataset:
	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

BART??

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)
	multi-group
	validation of indata to predict?

======================================================================================

PyBind11 issues:
	how translate custom C++ exceptions to standard Python exceptions
	no way of specifying noconvert for property setters
	how to set function attributes
	
Gerstmann issues:
	seed function would be useful
	free functions (operator== and operator!=) should be inline
		or linker will complain if you include the header in multiple translation units
	should be standard compliant

Eigen issues:
	can not reset Ref objects
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...

	Eigen::Ref<Eigen::ArrayXf> u;
	auto p1 = u.begin();
	auto p2 = begin(u);
	auto p3 = u.cbegin();
	auto p4 = cbegin(u);
	float* p5 = &u(0);

	Eigen::Ref<Eigen::ArrayXf> u;
	auto q1 = u.cbegin();
	auto q2 = cbegin(u);
	const float* q3 = &u(0);

	also reverse iterators

NumPy issues:
	inData[trainSamples, :] does not preserve Fortran (column major) order
		how to get fortran ordered slices of fortran ordered arrays?

Pandas issues:
	if you set dtype when calling read_csv(), this is applied to the index column as well 

------------------------------------------------------------------------

take a look at std::span, std::view, std::mdview, ranges etc in C++20

======================================================================================

Notes:

/O2 =  /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")


