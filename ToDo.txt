review TreeTrainerImpl one more time
	continue with initSampleStatus_

commit

improved profiling log
	group into unthreaded, partially threaded, fully threaded and thread synch

t-test and F-test:
	is the following analysis correct?
		if the data matrix fits in the cache we can parallelize and vectorize as much as possible
		but if the data matrix does not fit in the cache, then things are different
			each element in the data matrix as accessed twice
				we need locality so that the the element still is in the cache the second time it is accessed
	the following should handle all cases
		divde the variables into large blocks, one for each thread
			then divide each block into strips, each one cashe line wide
	faster with single precision for intermediate values? how much would precision be hurt?

------------------------------------------------------------------------------------------------------------------------

Test with the Intel C++ compiler:
	Does it have better support for 128 threads?
	Does it have better support for SIMD vectorization?
	To do:	
		1. report that vectors of overaligned data do not compile
		2. check the alignment of buffers of such vectors
		3. report internal compiler error to
			https://community.intel.com/t5/Intel-C-Compiler/bd-p/c-compiler
		4. test how well the compiler handles 128 threads and SIMD vectorization
		5. how well does the generated code work with AMD processors?

------------------------------------------------------------------------------------------------------------------------

major tasks:
	release 0.1 (reset file format version?)
	improved Higgs example (see "Higgs Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	BART
	improved Otto example (see "Otto Notes.txt")
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	histogram-based tree builder (would speed up Otto a lot)
	multinomial predictors
	documentation of regularized logit boost method

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	set seed to make the code deterministic (how to handle dynamic scheduling? doable, but a bit tricky)
	U-test (with correct handling of ties)
	multiple inData and multiple variable counts (to handle feature engineering)

optimizations (accuracy):
	test the new selectVariablesByLevel option
	boosted forests
		maybe train with forestSize = 1, then build with a larger forestSize
	boost stop criteria
	pruning
	multi-label stratification when generating folds and selecting active samples
	
optimizations (speed):
	make updateSampleStatus() code branch-free by introducing dummy nodes that conain unused samples
		maybe maintain sorted lists of active samples to speed up this further
		improved cost function when ordering options objects in parallel train
		higgs_quick.py is a suitable test case
	data:
		narrower type for strata
		outdata and weights interleaved in a single array
		outdata and weights as float?
	Schraudolph:
		_mm256_cvtpd_epi32   double to int32
		is there any int32 to int64 conversion?
		I may have to write my own code with intrinsics instead of relying on the autovectorizer
	parallelize predict function (omp parallellize ensemble and boost predictors, but only in nonparallel regions)
	maybe some cached ordered samples mechanism to use with usedVariableByLayer = true?
    narrower TreeTrainerImpl::SampleStatus??
	look into fast/precise sums
		does it make any difference for speed and accuracy?
	PGO
	test with the Intel C++ compiler (how does the generated code perform with AMD processors?)

minor things:
	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
	is there any need for the fudge term in the t- and F-test? (How large should the fudge term be?)
	save and load:
		check that dataset does not have more than 2G variables
		get rid of uint32 nonsense
	compare performance (speed and accuracy) with xgboost
	clang code formatting
	how fast can a column major implementation of the t-statistic be?
	look more into Eigen arrays and std::data
	seems pybind11 does not support arguments of types such as optional<reference_wrapper<vector<int>>>
		that would be useful for passing sample lists to t-test and F-test
