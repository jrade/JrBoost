
 major tasks:
	variable importance weights
	clean up Higgs and Otto
	BART
	histogram based stumps (would speed up Otto a lot)
	gradient boost

========================================================================

trees:

	how does the old tree trainer behave on the work computer?????

	do next:

		there are severel possible implementations of tree building

			dynamically updated datastructure:
				A. sample state (UNUSED = 0, node index offset by 1)
				B. sample state (UNUSED = (SampleIndex)-1, node index), filtered ordered samples
				C. filtered and groups ordered samples
				Notes: C can ot be updated inplace so requires more memory

			node builders can be online or batch

			do we need to test all combinations?????

			test A, B and C with batch builders
		

		ditch the stumps and clean up
			TreeOptions -> TreeOptions
				review friend declarations in class PROFILE
				make a permanent commit (it's about time)

		tweak the cost function?
				
		tree pruning

	..........................................................

	more efficient storage of usedSortedSamples? (by variable index instead of by variable)		
		would not work with a (yet to be implemented) select variables by level mode

	further optimizations:
		carry over sums?

	how to use tree trainer:
		decrease top variable count?
		increase cycle count?
	
	features to add:
		usedVariables per level option (may be expensive)

	code simplification:
		are static thread_local buffers worth the trouble?

============================================================================================

predict overload that that takes a one-dim array and returns a double (use in QD Demo)

is there any need for the fudge term in the t- and F-test? 
	How large should the fudge term be?

================================================================================================

possible optimizations
	test PGO again
	test narrower type for strata
	float or double for outdata and weights?
	optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
	alternative implementation of sortedUsedSamples
		select fastest implementation dynamically depending on sample count etc.
		maybe do this as a separate stump trainer class

documentation of regularized logit boost

new GitHub login method
	https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

Eigen:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles
	pass Ref<const Array> by const ref?


should score > bestScore be replaced by something stricter?
	make init value of bestScore slightly larger?

========================================================================
	
Higgs:
	set up to run with the original kaggle files

	two modes:
		cv and submitt to kaggle server
		nested cv
			test 3 outer and 2 inner folds
		load function with random sampling for testing

	use weights when building predictor


	do some profiling with different numbers of train samples
		how long does it take to load file?
			transform to binary format?

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc

	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

U-test (with correct handling of ties)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

multiple inData and multiple variableCount (to handle feature engineering etc.)

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)

	test accuracy of fast math
		1. improved fast math with smarter (x + 1) ^ (gamma-2)
		2. update the Gist with fastLog and fastPow
		3. plot min and max rel error of fastPow as a function of gamma
			let x range over 2 ** -n to 2 **n; seems the exact form of curve depends a little on on
			send a mail to Nic with the results

	better error messages when converting arguments to BoostTrainer::train

	profiling system that distinguishes between time in parallellized and non-parallellized code

	multinomial classification

======================================================================================
	
Eigen issues:
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

======================================================================================

Notes:

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
