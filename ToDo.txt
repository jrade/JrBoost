


Leukeima external vaidation, 10 reps x 5 folds, laptop:

........................................................

A:

main                            -nan(ind)  -nan(ind)%
  t-rank                        -nan(ind)  -nan(ind)%  -nan(ind)
  boost init                    -nan(ind)  -nan(ind)%
    init sorted samples         -nan(ind)  -nan(ind)%  -nan(ind)
  boost train                   -nan(ind)  -nan(ind)%  -nan(ind)
    stump train                 -nan(ind)  -nan(ind)%  -nan(ind)
      sums                      -nan(ind)  -nan(ind)%
     tree predict               -nan(ind)  -nan(ind)%  -nan(ind)
    thread synch                -nan(ind)  -nan(ind)%
  boost predict                 -nan(ind)  -nan(ind)%  -nan(ind)

profiling overhead: -nan(ind)%
slow branch: -nan(ind)%

0:06:32

.......................................................

C:

main                               11058   0.7%
  t-rank                           13775   0.9%    3.1
  boost init                          63   0.0%
    init sorted samples             2215   0.1%   18.9
  boost train                      27607   1.8%   21.3
    stump train                    31800   2.1%
      used variables                7902   0.5%   16.9
      init sample status           36795   2.4%   28.4  <------
      init used sorted samples    187822  12.5%    4.7  <-----
      update sample status         20401   1.4%   20.4
      update used sorted samples   95528   6.4%    4.6
      init ordered samples        249529  16.6%   12.5
      update node builder         510728  34.0%   15.7  <----
      sums                          5382   0.4%    6.2
     tree predict                  29267   1.9%   22.6
    thread synch                  263355  17.5%
  boost predict                     9338   0.6%  29255.4

profiling overhead: 14.1%
slow branch: 9.7%

0:14:20

................................................................................................

D:

main                               10664   0.9%
  t-rank                           13116   1.1%    2.9
  boost init                          61   0.0%
    init sorted samples             2137   0.2%   18.2
  boost train                      28232   2.5%   20.2
    stump train                    35355   3.1%
      used variables                8415   0.7%   15.9
      init sample mask             33678   2.9%   24.1  <------
      init ordered samples        111864   9.7%    2.4  <------
      update ordered samples      160983  14.0%    7.9
      update node builder         490722  42.7%   14.9  <------
      sums                          5645   0.5%    6.3
     tree predict                  29977   2.6%   21.4
    thread synch                  208215  18.1%
  boost predict                     9269   0.8%  26821.8

profiling overhead: 1.4%
slow branch: 10.6%

0:09:44


============================================================

 major tasks:
	variable importance weights
	clean up Higgs and Otto
	BART
	histogram based stumps (would speed up Otto a lot)
	gradient boost

========================================================================

TreeTrainer implementations:
  A. stumps
  B. sample state (UNUSED = 0, node index offset by 1)
  C. sample state (UNUSED = (SampleIndex)-1, node index), filtered ordered samples
  D. filtered and grouped ordered samples


trees:
	now:
		micro optimizations
			more branch-free code
		avoid accessing thread local vectors in inner loops in initSampleStatus() / initSampleMask() and other places
		clean up profiling code
		general code review

	this week:
		tree pruning

	later:
		test other parallellization strategy
			try D with a more finegrained paralllellization
			if this works, restructure to get the largest possible 
					for(size_t j: usedVariables) { .... }
				block
		resurrect TreeBuilderB by backporting from TreeBuilderC	

	..........................................................

	more efficient storage of usedSortedSamples, orderedSamples_ etc? (by variable index instead of by variable)		
		would not work with a (yet to be implemented) select variables by level mode

	further optimizations:
		carry over sums?

	how to use tree trainer:
		decrease top variable count?
		increase cycle count?
	
	features to add:
		usedVariables per level option (may be expensive)

	code simplification:
		are static thread_local buffers worth the trouble?

============================================================================================

predict overload that that takes a one-dim array and returns a double (use in QD Demo)

is there any need for the fudge term in the t- and F-test? 
	How large should the fudge term be?

================================================================================================

possible optimizations
	test PGO again
	test narrower type for strata
	float or double for outdata and weights?
	optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
	alternative implementation of sortedUsedSamples
		select fastest implementation dynamically depending on sample count etc.
		maybe do this as a separate stump trainer class

documentation of regularized logit boost

new GitHub login method
	https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

Eigen:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles
	pass Ref<const Array> by const ref?


should score > bestScore be replaced by something stricter?
	make init value of bestScore slightly larger?

========================================================================
	
Higgs:
	set up to run with the original kaggle files

	two modes:
		cv and submitt to kaggle server
		nested cv
			test 3 outer and 2 inner folds
		load function with random sampling for testing

	use weights when building predictor


	do some profiling with different numbers of train samples
		how long does it take to load file?
			transform to binary format?

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc

	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

U-test (with correct handling of ties)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

multiple inData and multiple variableCount (to handle feature engineering etc.)

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)

	test accuracy of fast math
		1. improved fast math with smarter (x + 1) ^ (gamma-2)
		2. update the Gist with fastLog and fastPow
		3. plot min and max rel error of fastPow as a function of gamma
			let x range over 2 ** -n to 2 **n; seems the exact form of curve depends a little on on
			send a mail to Nic with the results

	better error messages when converting arguments to BoostTrainer::train

	profiling system that distinguishes between time in parallellized and non-parallellized code

	multinomial classification

======================================================================================
	
Eigen issues:
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

======================================================================================

Notes:

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
