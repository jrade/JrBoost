one more general review

push to GitHub
	ask David to download, compile with other compilers

sedan

------------------------------------------------------------------------------------------------------------------------

Test with the Intel C++ compiler:
	Does it have better support for 128 threads?
	Does it have better support for SIMD vectorization?
	To do:	
		1. report that vectors of overaligned data do not compile
		2. check the alignment of buffers of such vectors
		3. report internal compiler error to
			https://community.intel.com/t5/Intel-C-Compiler/bd-p/c-compiler
		4. test how well the compiler handles 128 threads and SIMD vectorization
		5. how well does the generated code work with AMD processors?

------------------------------------------------------------------------------------------------------------------------

major tasks:
	release 0.1 (reset file format version?)
	improved Higgs example (see "Higgs Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	BART
	improved Otto example
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	histogram-based tree builder (would speed up Otto a lot)
	multinomial predictors
	documentation of regularized logit boost method

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	set seed to make the code deterministic (how to handle dynamic scheduling? doable, but a bit tricky)
	U-test (with correct handling of ties)
	multiple inData and multiple variable counts (to handle feature engineering)

optimizations (accuracy):
	boosted forests
		maybe train with forestSize = 1, then build with a larger forestSize
	pruning
	used variable subset by level option (would require saveMemory = True)
	multi-label stratification when generating folds and selecting active samples
	
optimizations (speed):
	improved profiling log
		group into unthreaded, partially threaded, fully threaded and thread synch
	t-test and F-test:
		is the following analysis correct?
			if the data matrix fits in the cache we can parallelize and vectorize as much as possible
			but if the data matrix does not fit in the cache, then things are different
				each element in the data matrix as accessed twice
					we need locality so that the the element still is in the cache the second time it is accessed
		the following should handle all cases
			divde the variables into large blocks, one for each thread
				then divide each block into strips, each one cashe line wide
	make updateSampleStatus() code branch-free by introducing dummy nodes corresponding to status = 0 (unusued)
		maybe maintain sorted lists of active samples to speed up this further
		improved cost function when ordering options objects in parallel train
	vectorization and parallelixation:
		parallelize predict function (omp parallellize ensemble and boost predictors, but only in nonparallel regions)
		joinNodeTrainers_ could be parallelized, but this requires cache line alignment of TreeNodeBuilder
			this seems not to work with the Intel compiler
		TreeTrainerImpl::train() - parallelize outside the d-loop
			profile with Leukemia and 64 threads
			does this make the code too messy?
	data:
		narrower type for strata
		float or double for outdata and weights?
		outData and weights interleaved in a single array
	rerun the profiling with different instruction sets and floating point flags
	compare performance with the old StumpTrainer
		if performance is the same or better, delete the old code
	PGO
	test with the Intel C++ compiler (how does the generated code perform with AMD processors?)

minor things:
	predict() overload that takes a one-dim array and returns a double (use in QD Demo)
	is there any need for the fudge term in the t- and F-test? (How large should the fudge term be?)
	save and load:
		check that dataset does not have more than 2G variables
		get rid of uint32 nonsense
	compare performance (speed and accuracy) with xgboost
	clang code formatting
	how fast can a column major implementation of the t-statistic be?
	look more into Eigen arrays and std::data
	seems PyBind11 does not support arguments of types such as optional<reference_wrapper<vector<int>>>
		that would be useful for passing sample lists to t-test and F-test
	simpler way of selecting used samples; pick each sample with probability usedSampleRatio, count may vary

------------------------------------------------------------------------------------------------------------------------

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?
	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc
	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

Eigen issues:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles but gives unexpected result
	no select with two constants
	    StumpTrainerImpl(CRefXXfc inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

------------------------------------------------------------------------------------------------------------------------

Notes:

The Visual Studio project uses the environment variables EIGEN_DIR, PYTHON_DIR and PYBIND11_DIR
to find third party header and library files

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
