
predict overload that that takes a onedim array abd return a double
is there any need for the fudge term in the t- and F-test? 

===============================================================================================

later: merge feature branch example-cleanup

trees
	for the time being, control with a preprocessor macro in BoostTrainer.h
		later add a dispatcher (based on tree depth parameter)

	allocate nodes with memory pool (use std::pmr::monotonic_buffer_resource)

	also pruning

================================================================================================

possible optimizations
	test PGO again
	test narrower type for strata
	float or double for outdata and weights?
	optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
	alternative implementation of sortedUsedSamples
		select fastest implementation dynamically depending on sample count etc.
		maybe do this as a separate stump trainer class

documentation of regularized logit boost

new GitHub login method
	https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

Eigen:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles
	pass Ref<const Array> by const ref?

========================================================================

need to move on with
	trees
	histogram based stumps (would speed up Otto a lot)
	gradient boost
	BART

========================================================================
	
Higgs:
	run with LOR and median
	plot the result
	draw a line in the plot that marks the estimated cutoff
	should we use a 0 cutoff?
	implement the AMS loss function
	run with CV - ignore the test datasets
	AUC as optimization target
		better cv + AUC implementation?
	percentage threshold

	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc

	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

=====================================================================

U-test (with correct handling of ties)

histogram based stumps (would speed up Otto a lot)

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

extended algorithms:
	first and second order boost
	alpha-beta boost with beta loss

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

multiple inData and multiple variableCount (to handle feature engineering etc.)

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)

	test accuracy of fast math
		1. improved fast math with smarter (x + 1) ^ (gamma-2)
		2. update the Gist with fastLog and fastPow
		3. plot min and max rel error of fastPow as a function of gamma
			let x range over 2 ** -n to 2 **n; seems the exact form of curve depends a little on on
			send a mail to Nic with the results

	better error messages when converting arguments to BoostTrainer::train

	profiling system that distinguishes between time in parallellized and non-parallellized code

	multinomial classification

======================================================================================
	
Eigen issues:
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

======================================================================================

Notes:

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
