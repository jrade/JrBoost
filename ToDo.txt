

main                      4021   3.2%
  t-rank                  4840   3.8%    4.4
  train boost            14853  11.8%   18.3
  train stumps
    used samples          7575   6.0%    9.3
    used variables        4033   3.2%   11.4
    sums                   485   0.4%    4.3
    sorted used s.       63398  50.2%    2.8
    best split           23799  18.8%    7.9
  predict                 1437   1.1%    7.1
  OMP barrier              294   0.2%
  dyn. memory             1669   1.3%  242.6

zero calibration: 77.4
profiling overhead: 25.1%
slow branch: 2.4%

0:01:19


---------------------------------------------------------------------

using size_t and double everywhere
	except:
		arrays of sampleIndices use smallest type
		inData: float

	profile the first decision again

=====================================================================

profile again, copy the numbers, check in

look again at CRef versus Ref, Ref works fine for strata

build single boostpredictor and check weights

find a suitable test data set
	Kaggle Gene Expression?
	check what datasets come with xgboost (would be great for comparison)

clean repo
push to GitHub

produce report
	first a simple one
	add a train and predict function similar to train and eval

Pandas free Python code

implement smarter hyperparameter optimization methods

-----------------------------------------------------------------------

gradient boost
	read xgb paper and Freiedman to check that my understanding is correct

trees
	allocate nodes with a memory pool
	pruning

data members of boosters should be const
	can not iterate over const Eigen Ref object!!!!

test PGO

extended algorithms:
	check if alpha boost idea works
	first and second order boost
	alpha boost
	alpha-beta boost with beta loss

review use of integer and foating point data types

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

histogram based methods

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)
	multi-group

----------------------------------------------------------------------

PyBind11 issues:
	how translate custom C++ exceptions to standard Python exceptions
	no way of specifying noocnvert for property setters
	investigate abstract factory crash (JrBooster Crash)
		is the problem return values that are derived objects passed with unique_ptr to base class?
		test with the Miniconda that somes bundles with Visual Studio
	
Gerstmann issues:
	seed function would be useful
	free functions (operator== and operator!=) should be inline
		or linker will complain if you include the header in multiple translation units
	should be standard compliant

Eigen issues:
	can not reset Ref objects
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...

	Eigen::Ref<Eigen::ArrayXf> u;
	auto p1 = u.begin();
	auto p2 = begin(u);
	auto p3 = u.cbegin();
	auto p4 = cbegin(u);
	float* p5 = &u(0);

	Eigen::Ref<Eigen::ArrayXf> u;
	auto q1 = u.cbegin();
	auto q2 = cbegin(u);
	const float* q3 = &u(0);

	also reverse iterators

NumPy issues:
	inData[trainSamples, :] does not preserve Fortran (column major) order

------------------------------------------------------------------------

take a look at std::span, std::view, std::mdview, ranges etc in C++20

