
make the profiling work properly again
	then profile with St Jude data on the 128 core PC (50% outer thread synch?)

better function names

profile threading overhead
	a bit tricky

review TreeTrainerImpl once more

......................................................................................................

report:

	MSVS autovectorizer:
		does not handle reinterpret cast
		if they fix this, then ditch all the Intel intrinsics code

	pybind11:
		seem py::kw_only() does not interact well with callbacks
			insert py::kw_only() in LogLoss::operator() and run Iris.py

	Eigen:
		why cannot we use (a < inf).all() to detect NaN values in Eigen arrays?
			using isFinite is very slow (considering we only need to inspect a few bits)
		why does std::data() not work here? (see Select.cpp)
			const float* pInDataRowI = std::data(inData.row(inI));
			float* pOutDataRowI = &outData(outI, 0);

.............................................................................
		
major tasks:
	improved higgs_precise.py (see also Higgs Notes.txt)
		use this to improve boost params in higgs_fast.py
		fit splines to AMS
		should use higg_util
	release 0.1
	BART
	documentation of the different boost methods, incuding regularized logit boost
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	compare performance (speed and accuracy) with xgboost
		construct Python classes with same interface as jrboost python classes
	histogram-based tree builder (use Otto as test case?)
	multinomial predictors
	what is the standard code organization for a Python extension module?

features to add:
	log boost parameter optimization process
		log quartiles of each parameter after each cycle
	U-test (with correct handling of ties)
	multiple indata and multiple variable counts (to handle advanced feature engineering and selection)
		applications: PCA, separate counts for up- and down-regulated variables, chromosomes, NCC
	new way of calculating variable importance weights: when predicting (instead of when training)
	have train, predict and loss functions take samples argument?

optimizations (accuracy):
	pruning
	iterate from both ends and meet in the middle when finding best split
	better parallelTrainAndEval for non-additive loss function (such as AUC)
	boost stop criteria
	other ways of regularizing logit boost (should be scale invariant)
	the current minNodeWeight option is absolute min node weight
		how about relaative min node weight (relative to sumW)?
	
optimizations (speed):
	parallelize updateSampleStatus (easier with PRECISE_SUMS = 0)
	t-test and F-test: are they SIMD optimized? do we load each cache line twice for large data sets?
	look into fast/precise sums; does it make any difference for speed and accuracy?
	Clang + LLVM + CMake for Visual Studio
		maybe no need then for IntelIntrinsics implementation of fastExp()
	PGO

keep or ditch?
	fine-grained stratification (does it improve accuracy?)
	py::arg().noconvert() in t-test and F-test python wrappers
	are the following options useful?
		saveMemory,
		stratifiedSamples
		cycle
	support for loading very old file format versions
