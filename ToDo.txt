packed data
	why does inner thread synch take so much time with packed data?
		do some more detailed profiling?
		what happens if there are no inner threads?
			push things to max with 128 threads, no inner threads on fast PC
	test with Higgs cv on Laptop
	if we keep it, don't pass around outData and weights

release 0.1
	reset the predictor file format version to 1 and ditch the backward compatibility code

------------------------------------------------------------------------------------------------------------------------

major tasks:
	improved Higgs example (see "Higgs Notes.txt")
	simpler high level API that does cross-validation and applies preprocessing
	documentation of the different boost methods, incuding regularized logit boost
	improved Otto example (see "Otto Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	histogram-based tree builder (would speed up Otto a lot, maybe Higgs too)
	BART
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	multinomial predictors

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	logit boost with   f' / (f'' + delta * f' * f') ?
	U-test (with correct handling of ties)
	multiple indata and multiple variable counts (to handle feature engineering)
	set seed to make the code deterministic (how to handle dynamic scheduling? doable, but a bit tricky)

optimizations (accuracy):
	better stratification when training multigroup classifiers
	better parallelTrainAndEval for non-additive loss function (such as AUC)
	boost stop criteria
	pruning
	multi-label stratification when generating folds and selecting active samples
	other ways of regularizing logit boost?
	
optimizations (speed):
	parallelize:
		ForestTrainer::train
		TreeTrainer::updateSampleStatus_
			easier if we do not use exact sums; can we do that?
		predict functions
	make branch free:
		TreeTrainer::updateSampleStatus_:
			introducing dummy nodes that contain the unused samples
			maybe maintain sorted lists of active samples to speed this up further
	t-test and F-test:
		divide the variables into large blocks, one for each thread
		then each blocks into vertical strips, each one __128 wide (to be converted to __m256d)
		look into fast/precise sums; does it make any difference for speed and accuracy?
	have boost trainer produce packed wy data and feed it to tree trainer (some SIMD swizzle magic needed?)
	PGO

minor things:
	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
	why cannot we use (a < inf).all() to detect NaN values in Eigen arrays?
	compare performance (speed and accuracy) with xgboost
	clang code formatting
	how fast can a column major implementation of the t-statistic be?
	seems pybind11 does not support arguments of types such as optional<reference_wrapper<vector<int>>>
		that would be useful for passing sample lists to t-test and F-test
	runtime checks for AVX2 and AVX512?
	Eigen:
		why does std::data() not work here? (see Select.cpp)
			const float* pInDataRowI = std::data(inData.row(inI));
			float* pOutDataRowI = &outData(outI, 0);
