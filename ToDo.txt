
next:

	review reindexing API - invert the index array?
		where is reindexing used?
	also do compactify when we are at it?

	commit

	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
		do complete implementations of both
		overload, does order matter for resolution
		try reducing one to the other both ways and profile
			multi-predict implemented using single-predict can easily be parallellized

	commit

	merge parallelTrainAndEval and parallelTrainAndEvalWeighted to one function?
		keyword only arguments
		need loss functions with optional weights (merge weighted and unweighted loss functions)
		then update Higgs to use the new train function
			add support for weights to train function
			need a wrapping train function that calculates the cutoff

	commit

	do a quick test test compiling with Clang for Visual Studio
		devblogs.microsoft.com/cppblog/clang-llvm-support-in-visual-studio/
		do we need the Intel Intrinsics to parallellize fastExp()?  Probably not


report:

	MSVS autovectorizer:
		does not handle reinterpret cast
		if they fix this, then ditch all the Intel intrinsics code

	pybind11:
		seems arguments of types such as optional<reference_wrapper<vector<int>>> are not supported
			that would be useful for passing sample lists to t-test and F-test

	Eigen:
		why cannot we use (a < inf).all() to detect NaN values in Eigen arrays?
		why does std::data() not work here? (see Select.cpp)
			const float* pInDataRowI = std::data(inData.row(inI));
			float* pOutDataRowI = &outData(outI, 0);

have BoostTrainer::BoostTrainer() take a samples argument?
	same for Predictor::predict()?
	use this to simplify jrboost::train() implementation

.............................................................................
		
major tasks:
	BART
	release 0.1
	documentation of the different boost methods, incuding regularized logit boost
	improved Higgs example (see "Higgs Notes.txt")
	improved Otto example (see "Otto Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	compare performance (speed and accuracy) with xgboost
		construct Python classes with same interface as jrboost python classes
	histogram-based tree builder (would speed up Otto a lot, maybe Higgs too)
	multinomial predictors
	Clang + CMake for Visual Studio
		what is the standard code organization for a Python extension module?

features to add:
	U-test (with correct handling of ties)
	multiple indata and multiple variable counts (to handle feature engineering)
		applications: PCA, separate counts for up- and down-regulated variables, chromosomes, NCC
	new way of calculating variable importance weights: when predicting (instead of when training)
	exception safe profiling

optimizations (accuracy):
	iterate from both ends and meet in the middle when finding best split
	better parallelTrainAndEval for non-additive loss function (such as AUC)
	boost stop criteria
	pruning
	other ways of regularizing logit boost (should be scale invariant)
	
optimizations (speed):
	test compiling with LLVM for Visual Studio
	parallelize:
		ForestTrainer::train
		TreeTrainer::updateSampleStatus_
			easier if we do not use exact sums; can we do that?
		predict functions
	make branch free:
		TreeTrainer::updateSampleStatus_:
			introducing dummy nodes that contain the unused samples
			maybe maintain sorted lists of active samples to speed this up further
	t-test and F-test: are they SIMD optimized? do we load each cache line twice for large data sets?
	look into fast/precise sums; does it make any difference for speed and accuracy?
	have boost trainer produce packed wy data and feed it to tree trainer (some SIMD swizzle magic needed?)
	PGO

keep or ditch?
	fine-grained stratification (does it improve accuracy?)
	packed data (does it improve speed?)
		if we keep it, don't pass around outData and weights
	py::arg().noconvert() in t-test and F-test python wrappers
	saveMemory option
	stratifiedSamples option
	support for loading old file format versions
