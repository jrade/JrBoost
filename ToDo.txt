
main                      1404   1.2%
  t-rank                  4436   3.9%    4.0
  train boost            13200  11.7%   15.8
  train stumps
    used samples          6897   6.1%    8.3
    used variables        3613   3.2%   10.0
    sums                   498   0.4%    4.1
    sorted used s.       56794  50.1%    2.5
    best split           23609  20.8%    7.3
  predict                 1245   1.1%    6.0
  OMP barrier              250   0.2%
  dyn. memory             1358   1.2%  192.5

zero calibration: 65.2
profiling overhead: 24.0%
slow branch: 2.5%

0:01:10

---------------------------------------------------------------------

using size_t and double everywhere
	except:
		arrays of sampleIndices use smallest type
		inData: float

	profile the first decision again

=====================================================================

Higgs dataset
	how to get AUC into the framework
		try simplified AUC
			do a branch free implementation

=====================================================================

start clenaup work uisng Iris
	
then push to GitHub

=====================================================================

produce report
	first a simple one
	add a train and predict function similar to train and eval

implement smarter hyperparameter optimization methods

better understanding of boosting:
	log high sample weight ratio
	log each sample after each boost iteration in a big spread sheet
	how does boosted trees behave if we only use trivial predictors?

-----------------------------------------------------------------------

Higgs:
	AUC as optimization target
	percentage threshold

trees
	allocate nodes with a memory pool
	pruning

gradient boost
	read xgb paper and Friedman to check that my understanding is correct

test PGO

extended algorithms:
	check if alpha boost idea works
	first and second order boost
	alpha boost
	alpha-beta boost with beta loss

review use of integer and foating point data types

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

histogram based methods

Kaggle Higgs dataset:
	maybe put together three things:
		winning contribution
		winning contribution with xgboost replaced by jrboost
		winning contribution with jrboost and smart training algorithm
		winning submission is written in lisp!!
			https://github.com/melisgl/higgsml

----------------------------------------------------------------------

low prio:
	look in the old stub (or rather tree) builder code and see how I avoided rounding off errors towards the end
	L2-penalty (lambda) or L1-penalty (alpha)
	multi-group

----------------------------------------------------------------------

PyBind11 issues:
	how translate custom C++ exceptions to standard Python exceptions
	no way of specifying noocnvert for property setters
	investigate abstract factory crash (JrBooster Crash)
		is the problem return values that are derived objects passed with unique_ptr to base class?
		test with the Miniconda that somes bundles with Visual Studio
	
Gerstmann issues:
	seed function would be useful
	free functions (operator== and operator!=) should be inline
		or linker will complain if you include the header in multiple translation units
	should be standard compliant

Eigen issues:
	can not reset Ref objects
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...

	Eigen::Ref<Eigen::ArrayXf> u;
	auto p1 = u.begin();
	auto p2 = begin(u);
	auto p3 = u.cbegin();
	auto p4 = cbegin(u);
	float* p5 = &u(0);

	Eigen::Ref<Eigen::ArrayXf> u;
	auto q1 = u.cbegin();
	auto q2 = cbegin(u);
	const float* q3 = &u(0);

	also reverse iterators

NumPy issues:
	inData[trainSamples, :] does not preserve Fortran (column major) order
		how to get fortran ordered slices of fortran ordered arrays?

------------------------------------------------------------------------

take a look at std::span, std::view, std::mdview, ranges etc in C++20

