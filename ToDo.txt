
TreeTrainerImpl:

	review TreeTrainerImpl one more time
		continue with initSampleStatus_()

	TreeNodeTrainer fork and join
		move join into finalize node builders (just as fork will be in initialize node builders)

	narrower SampleStatus type?
		templatized TreeTrainerImpl::train() function?
		move static threadlocal data to base class

	narrower sample strata type

	outdata and weights interleaved in a single array

	test fast bernoulli with r % n < k condition; profile

------------------------------------------------------------------------------------------------------------------------

Test with the Intel C++ compiler:
	Does it have better support for 128 threads?
	Does it have better support for SIMD vectorization?
	To do:	
		1. report that vectors of overaligned data do not compile
		2. check the alignment of buffers of such vectors
		3. report internal compiler error to
			https://community.intel.com/t5/Intel-C-Compiler/bd-p/c-compiler
		4. test how well the compiler handles 128 threads and SIMD vectorization
		5. how well does the generated code work with AMD processors?

------------------------------------------------------------------------------------------------------------------------

major tasks:
	release 0.1 (reset file format version?)
	improved Higgs example (see "Higgs Notes.txt")
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	BART
	improved Otto example (see "Otto Notes.txt")
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	histogram-based tree builder (would speed up Otto a lot)
	multinomial predictors
	documentation of regularized logit boost method

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	set seed to make the code deterministic (how to handle dynamic scheduling? doable, but a bit tricky)
	U-test (with correct handling of ties)
	multiple inData and multiple variable counts (to handle feature engineering)

optimizations (accuracy):
	test the new selectVariablesByLevel option
	boosted forests
		maybe train with forestSize = 1, then build with a larger forestSize
	boost stop criteria
	pruning
	multi-label stratification when generating folds and selecting active samples
	truncated logot boost?
	
optimizations (speed):
	improved profiling log
		group into unthreaded, partially threaded, fully threaded and thread synch
	t-test and F-test:
		divide the variables into large blocks, one for each thread
		then divide each block into __m256 columns
	make updateSampleStatus() code branch-free by introducing dummy nodes that conain unused samples
		maybe maintain sorted lists of active samples to speed up this further
	improved cost function when ordering options objects in parallel train
	outdata and weights as float??
	parallelize predict function (omp parallellize ensemble and boost predictors, but only in nonparallel regions)
	maybe some cached ordered samples mechanism to use with usedVariableByLayer = true?
	look into fast/precise sums; does it make any difference for speed and accuracy?
	submatrix functions perform poorly with too many threads? limit number of threads to say 8??
	can GCC/clang autovectorize the scalar fast exp loops in BoostTrainer::trainAda_() ?
		if so, we do not need the SIMD intrinsics code with GCC/clang
	PGO
	test with the Intel C++ compiler (how does the generated code perform with AMD processors?)

minor things:
	predict() overload that takes a one-dim array and returns a double (when classifying a single sample)
	is there any need for the fudge term in the t- and F-test? (How large should the fudge term be?)
	save and load:
		check that dataset does not have more than 2G variables
		get rid of uint32 nonsense?
	compare performance (speed and accuracy) with xgboost
	clang code formatting
	how fast can a column major implementation of the t-statistic be?
	look more into Eigen arrays and std::data
	seems pybind11 does not support arguments of types such as optional<reference_wrapper<vector<int>>>
		that would be useful for passing sample lists to t-test and F-test
	runtime checks for AVX2 and AVX512
