cleanups:
	variable weights as ArrayXd instead of vector<double>
	ordering of includes
	remove fastMath
	show ccc/item, item count and ccc count in profiling log
	clean up the slow branch profiling
	Rename the jrboost.py project Python, move it up one step in the solution explorer hierarchy
	decrease default gamma (in log loss) from 0.1 to 0.001?
	Predictor::_predict should work on an existing vector instead of returning a new vector each time
	clean up the profiling

always do update sample status?
	update sums at the same time?

classifier compactification

More systematic profiling cases:
	check for optimal thread count?
	Leukemia build-and-validate
	higgs.py and higgs_quick.py
	TCGA500 validate (only on desktop)

TreeBuilderImpl:

	do a careful comparison of the old and the new (discarded) code
		port everything that look useful
		check how the main tree builder loop is structured in the inner parallelized code

	use std::span to simplify code?

	general code review - are there things we should bring in from the inner parallel version?

	review the init/update ordered samples functions

	resurrect filtered ordered samples implementation (filter once or for each level?)
		could in fact be the best implementation

	profile the two implementations
		compare the detaled profile logs
		is the alt implementation faster?

	notes:
		the alt implementation can not be used with variables per level
		the alt implementation should not be used with depth 1 (would waste a lot of memory)

Minor things:
	test pruning again (also with abs limit)
	Trick to reduce rounding off errors towards the end
		calculate right sums as total - left instead of doing it incrementally
		then we should have right right sum == 0.0 at the end (assert this?)
	are static thread_local buffers worth the trouble? some of them? profile
	if we allowed depth = 0 (and why wouldn't we?), then the root would not get properly initialized
	let Split class find the splits??
		can also update the tree - see the TreeBuilderAlt / NodeBuilder code

------------------------------------------------------------------------------------------------------------------------

Higgs:

	feature engineerings
		read paper appendix A, then appendix B
		look at winning (and other) submissions

	target loss functions (for fitting hyper parameters):
		AUC:      3.7372 -> 3.7403
		AMS:      3.7340 -> 3.7342
		ll(1e-3): 3.7156 -> 3.7073
	
	more robust AMS cutoff fitting (testing every possible cutoff may lead to overfitting, apply some smoothing?)
	also: test simple cutoff again
	population optimization parameters (generator approach)
	repetition count

	minGain feature
	forward and backward pruning
	drop useless variables?

	cross-validation for fitting hyper parameters and AMS cutoff
		only use the data files train.csv and test.csv from Kaggle competition
		trainer that works with sample subset

	look at final version of my Kaggle code
	look at winning submissions, paper, code at https://github.com/melisgl/higgsml
	look at other publicly available submissions
		
	do some profiling
		how does it scale with number of cores (using the full dataset)
		what takes so much time at the end?

	compare with the xgbost higgs example:
		speed, accuracy?

	how does higgs.py find jrboost when importing it? (this ís a bit of a mystery)

------------------------------------------------------------------------------------------------------------------------

major tasks:
	improved Otto example
	gradient boost (Friedman and xgb papers)
		first and second order boost with any gamma
	BART
	other parallelization strategy
		the current version is threadsafe but non threaded
		first put together a simple non-threadsafe version
		then from there a threaded version
		finally a nested version
			https://software.intel.com/content/www/us/en/develop/videos/using-nested-parallelism-in-openmp.html
	histogram-based tree builder (would speed up Otto a lot)
	multinomial predictors

features to add:
	classifier compactification (remove unused variables, renumber (and reorder) the remaining variables)
	tree regularization: L2 penalty (lambda), L1 penalty (alpha)
	used variable subset by level option (may be expensive)
	predict overload that takes a one-dim array and returns a double (use in QD Demo)
	multiple inData and multiple variable counts (to handle feature engineering etc.)
	set seed (to make the training process deterministic)
	U-test (with correct handling of ties)

minor things:
	is there any need for the fudge term in the t- and F-test? 
		How large should the fudge term be?
	documentation of regularized logit boost
	review handling of rounding off errors towards end when splitting nodes (look in old code)
	clang code formatting
	what is the correct way of adding terms when calculating variable weights?

possible optimizations:
	test PGO again
	test narrower type for strata
	float or double for outdata and weights?
	optimize case when all samples are used (usedSampleRation = 1.0, minSampleWeight = 0.0)
	alternative implementation of sortedUsedSamples
		select fastest implementation dynamically depending on sample count etc.
		maybe do this as a separate stump trainer class

========================================================================================================================

Eigen:
	read Eigen 3.4 docs
	report that
			ArrayXf a = {0, 4, 5};
		compiles

------------------------------------------------------------------------------------------------------------------------

Otto:
	run an explorative round with full dataset
		usedSampleRatio = [0.01, 0.02, .... 1]
		minNodeSize = [1,2,5,10, 20, , .... 10000]
	test regularized log loss with some gamma around 0.1?

	fit hyperparams jointly or separately for the different labels?
	try again with linloss, logloss and auc

	otto with frac = 0.001, assertion weights >= 0 fails  --- investigate

------------------------------------------------------------------------------------------------------------------------

review use of integer and floating point data types
	use int64_t as default integer type?

stump train strategy:
	current method has time complexity (in clock cycles)
		3 * sampleCount * usedVariableCount + 10 * usedSampleCount * usedVariableCount
	there is an alternative method with time complexity
		c * usedSampleCount * log2(usedSampleCount) * usedVariableCount + 10 * usedSampleCount * usedVariableCunt
	create a used sample list and sort it each time
	this might be faster when usedSampleCount is small i.e. when we do weight filtering and most weights are small
	use pdq-sort (https://github.com/orlp/pdqsort) when sorting

------------------------------------------------------------------------------------------------------------------------
	
Eigen issues:
	no select with two constants
	    StumpTrainerImpl(CRefXXf inData, RefXs strata)
			second argument should be CRefXs, but that leads to problems ...
	is BoostTrainer::trainAda_ slower with Eigen 3.4 than with Eigen 3.3.9?

========================================================================================================================

Notes:

/O2 = /Og /Oi /Ot /Oy /Ob2 /GF /Gy

/GL /O2 and /Ot make a big difference

/arch:AVX is faster than /arch:SSE and /arch:SSE2
/arch:AVX2 is not faster than /arch:AVX
the sppedup comes from BoostTrainer::trainAda_() (possibly the exp() call)

/fp:strict, /fp:pecise, /fp:fast: no clear difference, but there is no reason not to choose /fp:fast

splitmix is the fastest rng

FastBernoulliDistribution: uint64_t is much faster than double
  turning the class into a function does not increase speed

other MSVS 2019 settings:
    Tools > Options > Python > Conda: ..../conda.exe
    compiler flag:  /permissive-
    Run-time Library = Multi-threaded DLL (/MD) (for all configurations!)
    mixed debugging

numpy arrays are by default row major ("C storage order")
Eigen arrays are by default column major ("Fortran storage order")
